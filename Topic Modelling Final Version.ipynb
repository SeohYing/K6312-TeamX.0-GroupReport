{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603b0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30937922",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = pd.read_csv('Coursera_courses.csv')\n",
    "reviews = pd.read_csv('Coursera_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d17dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviewers</th>\n",
       "      <th>date_reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>course_id</th>\n",
       "      <th>name</th>\n",
       "      <th>institution</th>\n",
       "      <th>course_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty dry, but I was able to pass with just t...</td>\n",
       "      <td>By Robert S</td>\n",
       "      <td>12-Feb-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would be a better experience if the video and ...</td>\n",
       "      <td>By Gabriel E R</td>\n",
       "      <td>28-Sep-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information was perfect! The program itself wa...</td>\n",
       "      <td>By Jacob D</td>\n",
       "      <td>08-Apr-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A few grammatical mistakes on test made me do ...</td>\n",
       "      <td>By Dale B</td>\n",
       "      <td>24-Feb-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent course and the training provided was...</td>\n",
       "      <td>By Sean G</td>\n",
       "      <td>18-Jun-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews       reviewers  \\\n",
       "0  Pretty dry, but I was able to pass with just t...     By Robert S   \n",
       "1  would be a better experience if the video and ...  By Gabriel E R   \n",
       "2  Information was perfect! The program itself wa...      By Jacob D   \n",
       "3  A few grammatical mistakes on test made me do ...       By Dale B   \n",
       "4  Excellent course and the training provided was...       By Sean G   \n",
       "\n",
       "  date_reviews  rating                 course_id  \\\n",
       "0    12-Feb-20       4  google-cbrs-cpi-training   \n",
       "1    28-Sep-20       4  google-cbrs-cpi-training   \n",
       "2    08-Apr-20       4  google-cbrs-cpi-training   \n",
       "3    24-Feb-20       4  google-cbrs-cpi-training   \n",
       "4    18-Jun-20       4  google-cbrs-cpi-training   \n",
       "\n",
       "                                                name  \\\n",
       "0  Become a CBRS Certified Professional Installer...   \n",
       "1  Become a CBRS Certified Professional Installer...   \n",
       "2  Become a CBRS Certified Professional Installer...   \n",
       "3  Become a CBRS Certified Professional Installer...   \n",
       "4  Become a CBRS Certified Professional Installer...   \n",
       "\n",
       "                 institution  \\\n",
       "0  Google - Spectrum Sharing   \n",
       "1  Google - Spectrum Sharing   \n",
       "2  Google - Spectrum Sharing   \n",
       "3  Google - Spectrum Sharing   \n",
       "4  Google - Spectrum Sharing   \n",
       "\n",
       "                                          course_url  \n",
       "0  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "1  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "2  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "3  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "4  https://www.coursera.org/learn/google-cbrs-cpi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(reviews,courses, on='course_id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a401af99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviewers</th>\n",
       "      <th>date_reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>course_id</th>\n",
       "      <th>name</th>\n",
       "      <th>institution</th>\n",
       "      <th>course_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty dry, but I was able to pass with just t...</td>\n",
       "      <td>By Robert S</td>\n",
       "      <td>12-Feb-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would be a better experience if the video and ...</td>\n",
       "      <td>By Gabriel E R</td>\n",
       "      <td>28-Sep-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information was perfect! The program itself wa...</td>\n",
       "      <td>By Jacob D</td>\n",
       "      <td>08-Apr-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A few grammatical mistakes on test made me do ...</td>\n",
       "      <td>By Dale B</td>\n",
       "      <td>24-Feb-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent course and the training provided was...</td>\n",
       "      <td>By Sean G</td>\n",
       "      <td>18-Jun-20</td>\n",
       "      <td>4</td>\n",
       "      <td>google-cbrs-cpi-training</td>\n",
       "      <td>Become a CBRS Certified Professional Installer...</td>\n",
       "      <td>Google - Spectrum Sharing</td>\n",
       "      <td>https://www.coursera.org/learn/google-cbrs-cpi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews       reviewers  \\\n",
       "0  Pretty dry, but I was able to pass with just t...     By Robert S   \n",
       "1  would be a better experience if the video and ...  By Gabriel E R   \n",
       "2  Information was perfect! The program itself wa...      By Jacob D   \n",
       "3  A few grammatical mistakes on test made me do ...       By Dale B   \n",
       "4  Excellent course and the training provided was...       By Sean G   \n",
       "\n",
       "  date_reviews  rating                 course_id  \\\n",
       "0    12-Feb-20       4  google-cbrs-cpi-training   \n",
       "1    28-Sep-20       4  google-cbrs-cpi-training   \n",
       "2    08-Apr-20       4  google-cbrs-cpi-training   \n",
       "3    24-Feb-20       4  google-cbrs-cpi-training   \n",
       "4    18-Jun-20       4  google-cbrs-cpi-training   \n",
       "\n",
       "                                                name  \\\n",
       "0  Become a CBRS Certified Professional Installer...   \n",
       "1  Become a CBRS Certified Professional Installer...   \n",
       "2  Become a CBRS Certified Professional Installer...   \n",
       "3  Become a CBRS Certified Professional Installer...   \n",
       "4  Become a CBRS Certified Professional Installer...   \n",
       "\n",
       "                 institution  \\\n",
       "0  Google - Spectrum Sharing   \n",
       "1  Google - Spectrum Sharing   \n",
       "2  Google - Spectrum Sharing   \n",
       "3  Google - Spectrum Sharing   \n",
       "4  Google - Spectrum Sharing   \n",
       "\n",
       "                                          course_url  \n",
       "0  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "1  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "2  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "3  https://www.coursera.org/learn/google-cbrs-cpi...  \n",
       "4  https://www.coursera.org/learn/google-cbrs-cpi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdata= df.loc[df.rating>=4]\n",
    "posdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a253dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviewers</th>\n",
       "      <th>date_reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>course_id</th>\n",
       "      <th>name</th>\n",
       "      <th>institution</th>\n",
       "      <th>course_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>This course is virtually worthless. I couldn't...</td>\n",
       "      <td>By James R</td>\n",
       "      <td>23-Apr-19</td>\n",
       "      <td>1</td>\n",
       "      <td>financial-markets-global</td>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>https://www.coursera.org/learn/financial-marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>This course was horrible! The Content was very...</td>\n",
       "      <td>By Sanam P</td>\n",
       "      <td>22-Oct-17</td>\n",
       "      <td>1</td>\n",
       "      <td>financial-markets-global</td>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>https://www.coursera.org/learn/financial-marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Terrible lecturer. Maybe he should retire. He ...</td>\n",
       "      <td>By Brendan F</td>\n",
       "      <td>29-Jan-19</td>\n",
       "      <td>1</td>\n",
       "      <td>financial-markets-global</td>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>https://www.coursera.org/learn/financial-marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>This course was aweful! I feel like I didnt le...</td>\n",
       "      <td>By Eli</td>\n",
       "      <td>18-Sep-17</td>\n",
       "      <td>1</td>\n",
       "      <td>financial-markets-global</td>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>https://www.coursera.org/learn/financial-marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>I was expecting more from the course. It is ve...</td>\n",
       "      <td>By Sebastian S</td>\n",
       "      <td>05-Jan-20</td>\n",
       "      <td>1</td>\n",
       "      <td>financial-markets-global</td>\n",
       "      <td>Financial Markets</td>\n",
       "      <td>Yale University</td>\n",
       "      <td>https://www.coursera.org/learn/financial-marke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviews       reviewers  \\\n",
       "87  This course is virtually worthless. I couldn't...      By James R   \n",
       "88  This course was horrible! The Content was very...      By Sanam P   \n",
       "89  Terrible lecturer. Maybe he should retire. He ...    By Brendan F   \n",
       "90  This course was aweful! I feel like I didnt le...          By Eli   \n",
       "91  I was expecting more from the course. It is ve...  By Sebastian S   \n",
       "\n",
       "   date_reviews  rating                 course_id               name  \\\n",
       "87    23-Apr-19       1  financial-markets-global  Financial Markets   \n",
       "88    22-Oct-17       1  financial-markets-global  Financial Markets   \n",
       "89    29-Jan-19       1  financial-markets-global  Financial Markets   \n",
       "90    18-Sep-17       1  financial-markets-global  Financial Markets   \n",
       "91    05-Jan-20       1  financial-markets-global  Financial Markets   \n",
       "\n",
       "        institution                                         course_url  \n",
       "87  Yale University  https://www.coursera.org/learn/financial-marke...  \n",
       "88  Yale University  https://www.coursera.org/learn/financial-marke...  \n",
       "89  Yale University  https://www.coursera.org/learn/financial-marke...  \n",
       "90  Yale University  https://www.coursera.org/learn/financial-marke...  \n",
       "91  Yale University  https://www.coursera.org/learn/financial-marke...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negdata= df.loc[df.rating<=2]\n",
    "negdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32011065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2841bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-6bcc910c8588>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posdata['clean_reviews'] = hero.clean(posdata['reviews'])\n"
     ]
    }
   ],
   "source": [
    "posdata['clean_reviews'] = hero.clean(posdata['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f29b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>clean_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pretty dry, but I was able to pass with just t...</td>\n",
       "      <td>pretty dry able pass two complete watches happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would be a better experience if the video and ...</td>\n",
       "      <td>would better experience video screen shots wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Information was perfect! The program itself wa...</td>\n",
       "      <td>information perfect program little annoying wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A few grammatical mistakes on test made me do ...</td>\n",
       "      <td>grammatical mistakes test made double take bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent course and the training provided was...</td>\n",
       "      <td>excellent course training provided detailed ea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0  Pretty dry, but I was able to pass with just t...   \n",
       "1  would be a better experience if the video and ...   \n",
       "2  Information was perfect! The program itself wa...   \n",
       "3  A few grammatical mistakes on test made me do ...   \n",
       "4  Excellent course and the training provided was...   \n",
       "\n",
       "                                       clean_reviews  \n",
       "0  pretty dry able pass two complete watches happ...  \n",
       "1  would better experience video screen shots wou...  \n",
       "2  information perfect program little annoying wa...  \n",
       "3     grammatical mistakes test made double take bad  \n",
       "4  excellent course training provided detailed ea...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdata[['reviews', 'clean_reviews']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "361597e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-20326fb77c26>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  negdata['clean_reviews'] = hero.clean(negdata['reviews'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>clean_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>This course is virtually worthless. I couldn't...</td>\n",
       "      <td>course virtually worthless follow lectures phd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>This course was horrible! The Content was very...</td>\n",
       "      <td>course horrible content disoriented unclear vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Terrible lecturer. Maybe he should retire. He ...</td>\n",
       "      <td>terrible lecturer maybe retire stay point lect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>This course was aweful! I feel like I didnt le...</td>\n",
       "      <td>course aweful feel like didnt learn anything u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>I was expecting more from the course. It is ve...</td>\n",
       "      <td>expecting course unstrucuted even misses compl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviews  \\\n",
       "87  This course is virtually worthless. I couldn't...   \n",
       "88  This course was horrible! The Content was very...   \n",
       "89  Terrible lecturer. Maybe he should retire. He ...   \n",
       "90  This course was aweful! I feel like I didnt le...   \n",
       "91  I was expecting more from the course. It is ve...   \n",
       "\n",
       "                                        clean_reviews  \n",
       "87  course virtually worthless follow lectures phd...  \n",
       "88  course horrible content disoriented unclear vi...  \n",
       "89  terrible lecturer maybe retire stay point lect...  \n",
       "90  course aweful feel like didnt learn anything u...  \n",
       "91  expecting course unstrucuted even misses compl...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negdata['clean_reviews'] = hero.clean(negdata['reviews'])\n",
    "negdata[['reviews', 'clean_reviews']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6a9d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d674ee933581>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posdata['clean_reviews'] = hero.remove_stopwords(posdata['clean_reviews'], custom_stopwords)\n",
      "<ipython-input-10-d674ee933581>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  negdata['clean_reviews'] = hero.remove_stopwords(negdata['clean_reviews'], custom_stopwords)\n"
     ]
    }
   ],
   "source": [
    "# We want to remove frequently occurring words which may not have meaning for analysis\n",
    "from texthero import stopwords\n",
    "default_stopwords = stopwords.DEFAULT\n",
    "#add a list of stopwords to the stopwords\n",
    "custom_stopwords = default_stopwords.union(set([\"coursera\",\"course\",\"really\",\"would\",\"also\",\"de\",\"curso\",\"que\",\"muy\",\"en\",\"el\"]))\n",
    "# Call remove_stopwords and pass the custom_stopwords list\n",
    "posdata['clean_reviews'] = hero.remove_stopwords(posdata['clean_reviews'], custom_stopwords)\n",
    "negdata['clean_reviews'] = hero.remove_stopwords(negdata['clean_reviews'], custom_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd5890",
   "metadata": {},
   "source": [
    "# Topic Modelling for Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2528124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e98a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "negdata_words = list(sent_to_words(negdata['clean_reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "463df0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['virtually', 'worthless', 'follow', 'lectures', 'phd', 'tenured', 'professor', 'almost', 'two', 'decades', 'online', 'based', 'snippets', 'taken', 'face', 'face', 'lectures', 'breakout', 'sessions', 'result', 'utter', 'incoherence', 'instructor', 'frequently', 'refers', 'concepts', 'yet', 'encountered', 'online', 'version', 'makes', 'extensive', 'use', 'mathematical', 'notations', 'formulas', 'whose', 'variables', 'never', 'properly', 'explained', 'generally', 'poor', 'job', 'explaining', 'material', 'online', 'version', 'yale', 'obviously', 'slapped', 'together', 'little', 'thought', 'production', 'value', 'one', 'unsuccessful', 'attempts', 'bring', 'revenue', 'cheap', 'creating', 'online', 'version', 'existing', 'face', 'face', 'material', 'rather', 'building', 'real', 'online', 'scratch', 'wasted', 'money', 'time', 'attempt', 'capitalize', 'yale', 'brand', 'pathetic']]\n"
     ]
    }
   ],
   "source": [
    "print(negdata_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb7cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virtually', 'worthless', 'follow', 'lectures', 'phd', 'tenured', 'professor', 'almost', 'two', 'decades', 'online', 'based', 'snippets', 'taken', 'face_face', 'lectures', 'breakout', 'sessions', 'result', 'utter', 'incoherence', 'instructor', 'frequently', 'refers', 'concepts', 'yet', 'encountered', 'online', 'version', 'makes', 'extensive', 'use', 'mathematical_notations', 'formulas', 'whose', 'variables', 'never', 'properly', 'explained', 'generally', 'poor', 'job', 'explaining', 'material', 'online', 'version', 'yale', 'obviously', 'slapped_together', 'little', 'thought', 'production', 'value', 'one', 'unsuccessful', 'attempts', 'bring', 'revenue', 'cheap', 'creating', 'online', 'version', 'existing', 'face_face', 'material', 'rather', 'building', 'real', 'online', 'scratch', 'wasted', 'money', 'time', 'attempt', 'capitalize', 'yale', 'brand', 'pathetic']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(negdata_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[negdata_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[negdata_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5ea5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "#def remove_stopwords(texts):\n",
    "#    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29076506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "#data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "negdata_words_bigrams = make_bigrams(negdata_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8f983e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpaCy in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (2.3.7)\r\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (7.4.5)\r\n",
      "Requirement already satisfied: setuptools in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (52.0.0.post20210125)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.0.6)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (0.7.5)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.0.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (4.59.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (0.8.2)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.1.3)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (3.0.6)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.0.6)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.0.5)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.20.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/evelynn/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (2.10)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "489415e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: spacy[en]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy[en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e9b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "691561c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e0a468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['virtually', 'worthless', 'follow', 'lecture', 'phd', 'tenure', 'almost', 'decade', 'online', 'base', 'snippet', 'take', 'face_face', 'lecture', 'breakout', 'session', 'result', 'utter', 'incoherence', 'instructor', 'frequently', 'refer', 'concept', 'yet', 'encounter', 'online', 'version', 'make', 'extensive', 'use', 'mathematical_notation', 'formula', 'variable', 'never', 'properly', 'explain', 'generally', 'poor', 'job', 'explain', 'material', 'obviously', 'slap', 'together', 'little', 'thought', 'production', 'value', 'unsuccessful', 'attempt', 'bring', 'revenue', 'cheap', 'create', 'online', 'version', 'exist', 'face_face', 'material', 'rather', 'build', 'real', 'online', 'scratch', 'waste', 'money', 'time', 'attempt', 'capitalize', 'pathetic']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "#python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "negdata_lemmatized = lemmatization(negdata_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(negdata_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b972a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 4), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 1), (59, 1), (60, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2wordneg = corpora.Dictionary(negdata_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "negtexts = negdata_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "negcorpus = [id2wordneg.doc2bow(text) for text in negtexts]\n",
    "\n",
    "# View\n",
    "print(negcorpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0863c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('almost', 1),\n",
       "  ('attempt', 2),\n",
       "  ('base', 1),\n",
       "  ('breakout', 1),\n",
       "  ('bring', 1),\n",
       "  ('build', 1),\n",
       "  ('capitalize', 1),\n",
       "  ('cheap', 1),\n",
       "  ('concept', 1),\n",
       "  ('create', 1),\n",
       "  ('decade', 1),\n",
       "  ('encounter', 1),\n",
       "  ('exist', 1),\n",
       "  ('explain', 2),\n",
       "  ('extensive', 1),\n",
       "  ('face_face', 2),\n",
       "  ('follow', 1),\n",
       "  ('formula', 1),\n",
       "  ('frequently', 1),\n",
       "  ('generally', 1),\n",
       "  ('incoherence', 1),\n",
       "  ('instructor', 1),\n",
       "  ('job', 1),\n",
       "  ('lecture', 2),\n",
       "  ('little', 1),\n",
       "  ('make', 1),\n",
       "  ('material', 2),\n",
       "  ('mathematical_notation', 1),\n",
       "  ('money', 1),\n",
       "  ('never', 1),\n",
       "  ('obviously', 1),\n",
       "  ('online', 4),\n",
       "  ('pathetic', 1),\n",
       "  ('phd', 1),\n",
       "  ('poor', 1),\n",
       "  ('production', 1),\n",
       "  ('properly', 1),\n",
       "  ('rather', 1),\n",
       "  ('real', 1),\n",
       "  ('refer', 1),\n",
       "  ('result', 1),\n",
       "  ('revenue', 1),\n",
       "  ('scratch', 1),\n",
       "  ('session', 1),\n",
       "  ('slap', 1),\n",
       "  ('snippet', 1),\n",
       "  ('take', 1),\n",
       "  ('tenure', 1),\n",
       "  ('thought', 1),\n",
       "  ('time', 1),\n",
       "  ('together', 1),\n",
       "  ('unsuccessful', 1),\n",
       "  ('use', 1),\n",
       "  ('utter', 1),\n",
       "  ('value', 1),\n",
       "  ('variable', 1),\n",
       "  ('version', 2),\n",
       "  ('virtually', 1),\n",
       "  ('waste', 1),\n",
       "  ('worthless', 1),\n",
       "  ('yet', 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2wordneg[id], freq) for id, freq in cp] for cp in negcorpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84532521",
   "metadata": {},
   "outputs": [],
   "source": [
    "neglda_model = gensim.models.ldamodel.LdaModel(corpus=negcorpus,\n",
    "                                           id2word=id2wordneg,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "415ea30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd100391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.298*\"time\" + 0.176*\"get\" + 0.155*\"explanation\" + 0.085*\"short\" + '\n",
      "  '0.075*\"waste\" + 0.047*\"worth\" + 0.037*\"teaching\" + 0.027*\"math\" + '\n",
      "  '0.026*\"big\" + 0.016*\"record\"'),\n",
      " (1,\n",
      "  '0.471*\"material\" + 0.200*\"little\" + 0.091*\"introduction\" + 0.084*\"subject\" '\n",
      "  '+ 0.045*\"job\" + 0.026*\"yet\" + 0.010*\"extensive\" + 0.001*\"encounter\" + '\n",
      "  '0.000*\"assignment\" + 0.000*\"know\"'),\n",
      " (2,\n",
      "  '0.664*\"good\" + 0.143*\"offer\" + 0.071*\"reading\" + 0.015*\"horrible\" + '\n",
      "  '0.000*\"assignment\" + 0.000*\"know\" + 0.000*\"lab\" + 0.000*\"test\" + '\n",
      "  '0.000*\"specialization\" + 0.000*\"require\"'),\n",
      " (3,\n",
      "  '0.397*\"use\" + 0.190*\"poor\" + 0.075*\"formula\" + 0.072*\"never\" + 0.054*\"base\" '\n",
      "  '+ 0.053*\"properly\" + 0.023*\"together\" + 0.022*\"version\" + 0.018*\"build\" + '\n",
      "  '0.018*\"exist\"'),\n",
      " (4,\n",
      "  '0.185*\"answer\" + 0.125*\"help\" + 0.106*\"ask\" + 0.094*\"try\" + 0.088*\"lack\" + '\n",
      "  '0.066*\"wrong\" + 0.043*\"apply\" + 0.035*\"other\" + 0.032*\"lose\" + '\n",
      "  '0.029*\"resource\"'),\n",
      " (5,\n",
      "  '0.240*\"poorly\" + 0.232*\"free\" + 0.140*\"issue\" + 0.137*\"clearly\" + '\n",
      "  '0.048*\"certification\" + 0.027*\"completion\" + 0.017*\"enroll\" + 0.004*\"cheat\" '\n",
      "  '+ 0.000*\"assignment\" + 0.000*\"test\"'),\n",
      " (6,\n",
      "  '0.104*\"knowledge\" + 0.095*\"difficult\" + 0.092*\"level\" + 0.089*\"beginner\" + '\n",
      "  '0.084*\"professor\" + 0.081*\"experience\" + 0.064*\"enough\" + 0.061*\"quite\" + '\n",
      "  '0.060*\"lesson\" + 0.055*\"long\"'),\n",
      " (7,\n",
      "  '0.372*\"lecture\" + 0.294*\"find\" + 0.186*\"expect\" + 0.059*\"stop\" + '\n",
      "  '0.012*\"flow\" + 0.004*\"loss\" + 0.000*\"assignment\" + 0.000*\"know\" + '\n",
      "  '0.000*\"test\" + 0.000*\"specialization\"'),\n",
      " (8,\n",
      "  '0.182*\"video\" + 0.154*\"learn\" + 0.114*\"content\" + 0.112*\"well\" + '\n",
      "  '0.106*\"much\" + 0.087*\"could\" + 0.074*\"way\" + 0.030*\"detail\" + '\n",
      "  '0.028*\"presentation\" + 0.017*\"structure\"'),\n",
      " (9,\n",
      "  '0.119*\"lot\" + 0.112*\"explain\" + 0.111*\"understand\" + 0.086*\"concept\" + '\n",
      "  '0.048*\"review\" + 0.044*\"new\" + 0.037*\"language\" + 0.029*\"simple\" + '\n",
      "  '0.028*\"almost\" + 0.028*\"star\"'),\n",
      " (10,\n",
      "  '0.339*\"want\" + 0.099*\"create\" + 0.093*\"drop\" + 0.088*\"can\" + 0.073*\"view\" + '\n",
      "  '0.065*\"business\" + 0.048*\"regard\" + 0.046*\"one\" + 0.036*\"pace\" + '\n",
      "  '0.006*\"principle\"'),\n",
      " (11,\n",
      "  '0.280*\"quiz\" + 0.227*\"question\" + 0.181*\"week\" + 0.113*\"bad\" + 0.041*\"case\" '\n",
      "  '+ 0.034*\"certain\" + 0.033*\"pass\" + 0.031*\"design\" + 0.016*\"compare\" + '\n",
      "  '0.006*\"related\"'),\n",
      " (12,\n",
      "  '0.326*\"need\" + 0.262*\"many\" + 0.218*\"say\" + 0.095*\"high\" + 0.015*\"dull\" + '\n",
      "  '0.012*\"self\" + 0.000*\"assignment\" + 0.000*\"know\" + 0.000*\"lab\" + '\n",
      "  '0.000*\"test\"'),\n",
      " (13,\n",
      "  '0.076*\"teach\" + 0.074*\"work\" + 0.070*\"even\" + 0.065*\"example\" + '\n",
      "  '0.065*\"think\" + 0.062*\"feel\" + 0.061*\"go\" + 0.060*\"student\" + 0.050*\"thing\" '\n",
      "  '+ 0.048*\"first\"'),\n",
      " (14,\n",
      "  '0.110*\"make\" + 0.100*\"take\" + 0.080*\"give\" + 0.057*\"cover\" + 0.053*\"topic\" '\n",
      "  '+ 0.050*\"provide\" + 0.045*\"class\" + 0.036*\"people\" + 0.034*\"follow\" + '\n",
      "  '0.034*\"look\"'),\n",
      " (15,\n",
      "  '0.109*\"basic\" + 0.108*\"however\" + 0.103*\"seem\" + 0.076*\"great\" + '\n",
      "  '0.074*\"interesting\" + 0.069*\"useful\" + 0.061*\"clear\" + 0.052*\"focus\" + '\n",
      "  '0.050*\"extremely\" + 0.047*\"different\"'),\n",
      " (16,\n",
      "  '0.187*\"instructor\" + 0.173*\"information\" + 0.107*\"hard\" + 0.092*\"present\" + '\n",
      "  '0.082*\"point\" + 0.053*\"come\" + 0.044*\"discussion\" + 0.040*\"due\" + '\n",
      "  '0.036*\"theory\" + 0.035*\"important\"'),\n",
      " (17,\n",
      "  '0.307*\"course\" + 0.270*\"complete\" + 0.177*\"certificate\" + 0.059*\"fast\" + '\n",
      "  '0.036*\"receive\" + 0.034*\"attempt\" + 0.023*\"amount\" + 0.010*\"respond\" + '\n",
      "  '0.000*\"assignment\" + 0.000*\"specialization\"'),\n",
      " (18,\n",
      "  '0.215*\"part\" + 0.109*\"instead\" + 0.106*\"money\" + 0.090*\"disappoint\" + '\n",
      "  '0.087*\"include\" + 0.051*\"multiple\" + 0.040*\"expectation\" + '\n",
      "  '0.040*\"technical\" + 0.040*\"change\" + 0.031*\"fundamental\"'),\n",
      " (19,\n",
      "  '0.000*\"therapy\" + 0.000*\"mesh\" + 0.000*\"intervention\" + 0.000*\"remainder\" + '\n",
      "  '0.000*\"town\" + 0.000*\"cancer\" + 0.000*\"supervise\" + 0.000*\"forefather\" + '\n",
      "  '0.000*\"andre\" + 0.000*\"streaming\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(neglda_model.print_topics())\n",
    "negdoc_lda = neglda_model[negcorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eaf7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "082b7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -17.14436904658859\n",
      "\n",
      "Coherence Score:  0.36178823302131663\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', neglda_model.log_perplexity(negcorpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "negcoherence_model_lda = CoherenceModel(model=neglda_model, texts=negdata_lemmatized, dictionary=id2wordneg, coherence='c_v')\n",
    "negcoherence_lda = negcoherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', negcoherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64cd5890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>[virtually, worthless, follow, lecture, phd, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>[horrible, content, disorient, unclear, video,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1378</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>[terrible, lecturer, maybe, retire, stay, poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>video, learn, content, well, much, could, way,...</td>\n",
       "      <td>[aweful, feel, learn, useful, information, sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.3419</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>[expect, unstrucuted, even, miss, complete, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>[interesting, need, self, explanatory, real_wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>[seem, connected, flow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>[terrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>[poorly, produce]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>[find, understand, slow, making, point, listen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            14.0              0.1660   \n",
       "1            1            14.0              0.1471   \n",
       "2            2            14.0              0.1378   \n",
       "3            3             8.0              0.1307   \n",
       "4            4            13.0              0.3419   \n",
       "5            5            13.0              0.1381   \n",
       "6            6            13.0              0.1278   \n",
       "7            7            13.0              0.1301   \n",
       "8            8            13.0              0.1301   \n",
       "9            9            14.0              0.1448   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  make, take, give, cover, topic, provide, class...   \n",
       "1  make, take, give, cover, topic, provide, class...   \n",
       "2  make, take, give, cover, topic, provide, class...   \n",
       "3  video, learn, content, well, much, could, way,...   \n",
       "4  teach, work, even, example, think, feel, go, s...   \n",
       "5  teach, work, even, example, think, feel, go, s...   \n",
       "6  teach, work, even, example, think, feel, go, s...   \n",
       "7  teach, work, even, example, think, feel, go, s...   \n",
       "8  teach, work, even, example, think, feel, go, s...   \n",
       "9  make, take, give, cover, topic, provide, class...   \n",
       "\n",
       "                                                Text  \n",
       "0  [virtually, worthless, follow, lecture, phd, t...  \n",
       "1  [horrible, content, disorient, unclear, video,...  \n",
       "2  [terrible, lecturer, maybe, retire, stay, poin...  \n",
       "3  [aweful, feel, learn, useful, information, sup...  \n",
       "4  [expect, unstrucuted, even, miss, complete, ta...  \n",
       "5  [interesting, need, self, explanatory, real_wo...  \n",
       "6                            [seem, connected, flow]  \n",
       "7                                         [terrible]  \n",
       "8                                  [poorly, produce]  \n",
       "9  [find, understand, slow, making, point, listen...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=neglda_model, corpus=negcorpus, texts=negdata):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[negcorpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(negtexts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "negdf_topic_sents_keywords = format_topics_sentences(ldamodel=neglda_model, corpus=negcorpus, texts=negdata)\n",
    "\n",
    "# Format\n",
    "negdf_dominant_topic = negdf_topic_sents_keywords.reset_index()\n",
    "negdf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "negdf_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38626aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>video, learn, content, well, much, could, way,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>teach, work, even, example, think, feel, go, s...</td>\n",
       "      <td>4209.0</td>\n",
       "      <td>0.1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>make, take, give, cover, topic, provide, class...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0            14.0  make, take, give, cover, topic, provide, class...   \n",
       "1.0            14.0  make, take, give, cover, topic, provide, class...   \n",
       "2.0            14.0  make, take, give, cover, topic, provide, class...   \n",
       "3.0             8.0  video, learn, content, well, much, could, way,...   \n",
       "4.0            13.0  teach, work, even, example, think, feel, go, s...   \n",
       "5.0            13.0  teach, work, even, example, think, feel, go, s...   \n",
       "6.0            13.0  teach, work, even, example, think, feel, go, s...   \n",
       "7.0            13.0  teach, work, even, example, think, feel, go, s...   \n",
       "8.0            13.0  teach, work, even, example, think, feel, go, s...   \n",
       "9.0            14.0  make, take, give, cover, topic, provide, class...   \n",
       "\n",
       "     Num_Documents  Perc_Documents  \n",
       "0.0            7.0          0.0003  \n",
       "1.0            NaN             NaN  \n",
       "2.0            NaN             NaN  \n",
       "3.0            NaN             NaN  \n",
       "4.0            6.0          0.0003  \n",
       "5.0            3.0          0.0001  \n",
       "6.0           18.0          0.0008  \n",
       "7.0            NaN             NaN  \n",
       "8.0         4209.0          0.1780  \n",
       "9.0          198.0          0.0084  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = negdf_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = negdf_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "negdf_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "negdf_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "negdf_dominant_topics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e5037",
   "metadata": {},
   "source": [
    "# Topic Modeling for positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9c27edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e7c80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "posdata_words = list(sent_to_words(posdata['clean_reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "874596ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pretty', 'dry', 'able', 'pass', 'two', 'complete', 'watches', 'happy', 'usual', 'questions', 'final', 'exam', 'annoying', 'far', 'better', 'many', 'microsoft', 'tests', 'taken', 'never', 'found', 'suplimental', 'material', 'references', 'cares', 'passed']]\n"
     ]
    }
   ],
   "source": [
    "print(posdata_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b97075a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretty', 'dry', 'able', 'pass', 'two', 'complete', 'watches', 'happy', 'usual', 'questions', 'final', 'exam', 'annoying', 'far', 'better', 'many', 'microsoft', 'tests', 'taken', 'never', 'found', 'suplimental', 'material', 'references', 'cares', 'passed']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(posdata_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[posdata_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[posdata_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d6f8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "#def remove_stopwords(texts):\n",
    "#    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99126cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "#data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "posdata_words_bigrams = make_bigrams(posdata_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c57a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pretty', 'dry', 'able', 'pass', 'complete', 'watch', 'happy', 'usual', 'question', 'final', 'exam', 'annoy', 'far', 'well', 'many', 'microsoft', 'test', 'take', 'never', 'find', 'suplimental', 'material', 'reference', 'care', 'pass']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "#python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "posdata_lemmatized = lemmatization(posdata_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(posdata_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dcab433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2wordpos = corpora.Dictionary(posdata_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "postexts = posdata_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "poscorpus = [id2wordpos.doc2bow(text) for text in postexts]\n",
    "\n",
    "# View\n",
    "print(poscorpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34853eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('able', 1),\n",
       "  ('annoy', 1),\n",
       "  ('care', 1),\n",
       "  ('complete', 1),\n",
       "  ('dry', 1),\n",
       "  ('exam', 1),\n",
       "  ('far', 1),\n",
       "  ('final', 1),\n",
       "  ('find', 1),\n",
       "  ('happy', 1),\n",
       "  ('many', 1),\n",
       "  ('material', 1),\n",
       "  ('microsoft', 1),\n",
       "  ('never', 1),\n",
       "  ('pass', 2),\n",
       "  ('pretty', 1),\n",
       "  ('question', 1),\n",
       "  ('reference', 1),\n",
       "  ('suplimental', 1),\n",
       "  ('take', 1),\n",
       "  ('test', 1),\n",
       "  ('usual', 1),\n",
       "  ('watch', 1),\n",
       "  ('well', 1)]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2wordpos[id], freq) for id, freq in cp] for cp in poscorpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43bcd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "poslda_model = gensim.models.ldamodel.LdaModel(corpus=poscorpus,\n",
    "                                           id2word=id2wordpos,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c4a1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21de3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.756*\"good\" + 0.153*\"basic\" + 0.038*\"online\" + 0.030*\"background\" + '\n",
      "  '0.014*\"training\" + 0.000*\"love\" + 0.000*\"project\" + 0.000*\"amazing\" + '\n",
      "  '0.000*\"finance\" + 0.000*\"problem\"'),\n",
      " (1,\n",
      "  '0.262*\"interesting\" + 0.186*\"use\" + 0.114*\"think\" + 0.088*\"different\" + '\n",
      "  '0.059*\"point\" + 0.054*\"aspect\" + 0.046*\"bit\" + 0.043*\"view\" + '\n",
      "  '0.031*\"already\" + 0.024*\"still\"'),\n",
      " (2,\n",
      "  '0.202*\"helpful\" + 0.092*\"quite\" + 0.090*\"question\" + 0.070*\"end\" + '\n",
      "  '0.065*\"include\" + 0.061*\"structured\" + 0.052*\"various\" + 0.052*\"answer\" + '\n",
      "  '0.050*\"base\" + 0.039*\"enough\"'),\n",
      " (3,\n",
      "  '0.402*\"great\" + 0.290*\"learn\" + 0.075*\"learning\" + 0.055*\"clear\" + '\n",
      "  '0.032*\"assignment\" + 0.027*\"opportunity\" + 0.026*\"review\" + 0.021*\"process\" '\n",
      "  '+ 0.019*\"wish\" + 0.019*\"peer\"'),\n",
      " (4,\n",
      "  '0.559*\"well\" + 0.285*\"material\" + 0.144*\"present\" + 0.000*\"love\" + '\n",
      "  '0.000*\"project\" + 0.000*\"amazing\" + 0.000*\"finance\" + 0.000*\"problem\" + '\n",
      "  '0.000*\"management\" + 0.000*\"course\"'),\n",
      " (5,\n",
      "  '0.397*\"give\" + 0.349*\"useful\" + 0.149*\"understanding\" + 0.092*\"important\" + '\n",
      "  '0.000*\"love\" + 0.000*\"project\" + 0.000*\"amazing\" + 0.000*\"finance\" + '\n",
      "  '0.000*\"management\" + 0.000*\"problem\"'),\n",
      " (6,\n",
      "  '0.114*\"topic\" + 0.094*\"know\" + 0.086*\"introduction\" + 0.065*\"go\" + '\n",
      "  '0.059*\"even\" + 0.058*\"cover\" + 0.049*\"level\" + 0.048*\"interested\" + '\n",
      "  '0.032*\"job\" + 0.032*\"worth\"'),\n",
      " (7,\n",
      "  '0.255*\"excellent\" + 0.147*\"experience\" + 0.105*\"instructor\" + 0.103*\"world\" '\n",
      "  '+ 0.065*\"idea\" + 0.056*\"able\" + 0.050*\"overall\" + 0.038*\"complex\" + '\n",
      "  '0.037*\"detailed\" + 0.032*\"appreciate\"'),\n",
      " (8,\n",
      "  '0.155*\"lot\" + 0.131*\"recommend\" + 0.075*\"highly\" + 0.073*\"video\" + '\n",
      "  '0.062*\"practical\" + 0.054*\"teach\" + 0.044*\"student\" + 0.037*\"see\" + '\n",
      "  '0.037*\"quiz\" + 0.036*\"simple\"'),\n",
      " (9,\n",
      "  '0.223*\"time\" + 0.217*\"explain\" + 0.156*\"need\" + 0.133*\"feel\" + '\n",
      "  '0.110*\"complete\" + 0.077*\"put\" + 0.053*\"difficult\" + 0.019*\"spend\" + '\n",
      "  '0.000*\"love\" + 0.000*\"project\"'),\n",
      " (10,\n",
      "  '0.265*\"way\" + 0.241*\"information\" + 0.181*\"many\" + 0.116*\"people\" + '\n",
      "  '0.045*\"step\" + 0.038*\"far\" + 0.030*\"never\" + 0.028*\"happy\" + 0.028*\"pretty\" '\n",
      "  '+ 0.017*\"basis\"'),\n",
      " (11,\n",
      "  '0.321*\"thing\" + 0.149*\"little\" + 0.129*\"design\" + 0.116*\"resource\" + '\n",
      "  '0.114*\"add\" + 0.101*\"read\" + 0.031*\"improvement\" + 0.017*\"fine\" + '\n",
      "  '0.000*\"love\" + 0.000*\"project\"'),\n",
      " (12,\n",
      "  '0.185*\"could\" + 0.130*\"beginner\" + 0.111*\"issue\" + 0.079*\"may\" + '\n",
      "  '0.078*\"change\" + 0.061*\"perfect\" + 0.059*\"expect\" + 0.058*\"introduce\" + '\n",
      "  '0.048*\"program\" + 0.030*\"believe\"'),\n",
      " (13,\n",
      "  '0.164*\"new\" + 0.163*\"get\" + 0.133*\"want\" + 0.122*\"week\" + 0.080*\"first\" + '\n",
      "  '0.052*\"especially\" + 0.034*\"day\" + 0.031*\"absolutely\" + 0.030*\"relate\" + '\n",
      "  '0.029*\"one\"'),\n",
      " (14,\n",
      "  '0.313*\"easy\" + 0.231*\"lecture\" + 0.229*\"provide\" + 0.131*\"follow\" + '\n",
      "  '0.029*\"thorough\" + 0.019*\"exam\" + 0.015*\"contain\" + 0.013*\"bad\" + '\n",
      "  '0.006*\"prepared\" + 0.000*\"love\"'),\n",
      " (15,\n",
      "  '0.216*\"help\" + 0.165*\"informative\" + 0.157*\"content\" + 0.084*\"study\" + '\n",
      "  '0.074*\"real\" + 0.068*\"case\" + 0.061*\"life\" + 0.033*\"forward\" + '\n",
      "  '0.031*\"career\" + 0.024*\"certificate\"'),\n",
      " (16,\n",
      "  '0.577*\"thank\" + 0.308*\"much\" + 0.053*\"gain\" + 0.053*\"teaching\" + '\n",
      "  '0.000*\"love\" + 0.000*\"amazing\" + 0.000*\"project\" + 0.000*\"problem\" + '\n",
      "  '0.000*\"management\" + 0.000*\"finance\"'),\n",
      " (17,\n",
      "  '0.182*\"find\" + 0.164*\"start\" + 0.119*\"future\" + 0.098*\"teacher\" + '\n",
      "  '0.093*\"skill\" + 0.069*\"engage\" + 0.069*\"develop\" + 0.045*\"presentation\" + '\n",
      "  '0.036*\"amount\" + 0.031*\"specific\"'),\n",
      " (18,\n",
      "  '0.153*\"knowledge\" + 0.152*\"take\" + 0.114*\"enjoy\" + 0.087*\"professor\" + '\n",
      "  '0.057*\"subject\" + 0.046*\"awesome\" + 0.045*\"professional\" + 0.035*\"create\" + '\n",
      "  '0.035*\"like\" + 0.032*\"depth\"'),\n",
      " (19,\n",
      "  '0.141*\"understand\" + 0.126*\"make\" + 0.090*\"concept\" + 0.083*\"example\" + '\n",
      "  '0.079*\"work\" + 0.055*\"class\" + 0.035*\"explanation\" + 0.032*\"detail\" + '\n",
      "  '0.026*\"come\" + 0.024*\"focus\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(poslda_model.print_topics())\n",
    "posdoc_lda = poslda_model[poscorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c26ddb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7689fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -15.882116099368028\n",
      "\n",
      "Coherence Score:  0.5551174898537996\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', poslda_model.log_perplexity(poscorpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "poscoherence_model_lda = CoherenceModel(model=poslda_model, texts=posdata_lemmatized, dictionary=id2wordpos, coherence='c_v')\n",
    "poscoherence_lda = poscoherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', poscoherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a9a68e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>understand, make, concept, example, work, clas...</td>\n",
       "      <td>[pretty, dry, able, pass, complete, watch, hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>excellent, experience, instructor, world, idea...</td>\n",
       "      <td>[well, experience, video, screen_shot, side, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1031</td>\n",
       "      <td>lot, recommend, highly, video, practical, teac...</td>\n",
       "      <td>[information, perfect, program, little, annoyi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>understand, make, concept, example, work, clas...</td>\n",
       "      <td>[grammatical, mistake, test, make, double, tak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>[excellent, training, provide, detailed, easy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>lot, recommend, highly, video, practical, teac...</td>\n",
       "      <td>[quiz, contain, material, explicitly, cover]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>lot, recommend, highly, video, practical, teac...</td>\n",
       "      <td>[solid, way, appreciate, intermittent, questio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>[probably, good, certification, take, respect,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>[proctoru, com, system, take, time, amount, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>[covered, required, information, way, video, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            19.0              0.0873   \n",
       "1            1             7.0              0.1313   \n",
       "2            2             8.0              0.1031   \n",
       "3            3            19.0              0.1030   \n",
       "4            4             3.0              0.0995   \n",
       "5            5             8.0              0.1048   \n",
       "6            6             8.0              0.1079   \n",
       "7            7             3.0              0.0985   \n",
       "8            8             3.0              0.0905   \n",
       "9            9             3.0              0.1375   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  understand, make, concept, example, work, clas...   \n",
       "1  excellent, experience, instructor, world, idea...   \n",
       "2  lot, recommend, highly, video, practical, teac...   \n",
       "3  understand, make, concept, example, work, clas...   \n",
       "4  great, learn, learning, clear, assignment, opp...   \n",
       "5  lot, recommend, highly, video, practical, teac...   \n",
       "6  lot, recommend, highly, video, practical, teac...   \n",
       "7  great, learn, learning, clear, assignment, opp...   \n",
       "8  great, learn, learning, clear, assignment, opp...   \n",
       "9  great, learn, learning, clear, assignment, opp...   \n",
       "\n",
       "                                                Text  \n",
       "0  [pretty, dry, able, pass, complete, watch, hap...  \n",
       "1  [well, experience, video, screen_shot, side, t...  \n",
       "2  [information, perfect, program, little, annoyi...  \n",
       "3  [grammatical, mistake, test, make, double, tak...  \n",
       "4  [excellent, training, provide, detailed, easy,...  \n",
       "5       [quiz, contain, material, explicitly, cover]  \n",
       "6  [solid, way, appreciate, intermittent, questio...  \n",
       "7  [probably, good, certification, take, respect,...  \n",
       "8  [proctoru, com, system, take, time, amount, ti...  \n",
       "9  [covered, required, information, way, video, g...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=poslda_model, corpus=poscorpus, texts=posdata):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[poscorpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(postexts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "posdf_topic_sents_keywords = format_topics_sentences(ldamodel=poslda_model, corpus=poscorpus, texts=posdata)\n",
    "\n",
    "# Format\n",
    "posdf_dominant_topic = posdf_topic_sents_keywords.reset_index()\n",
    "posdf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "posdf_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bc9edb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>understand, make, concept, example, work, clas...</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>excellent, experience, instructor, world, idea...</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>lot, recommend, highly, video, practical, teac...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>understand, make, concept, example, work, clas...</td>\n",
       "      <td>697589.0</td>\n",
       "      <td>0.7053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>lot, recommend, highly, video, practical, teac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>lot, recommend, highly, video, practical, teac...</td>\n",
       "      <td>953.0</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>157158.0</td>\n",
       "      <td>0.1589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>great, learn, learning, clear, assignment, opp...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0            19.0  understand, make, concept, example, work, clas...   \n",
       "1.0             7.0  excellent, experience, instructor, world, idea...   \n",
       "2.0             8.0  lot, recommend, highly, video, practical, teac...   \n",
       "3.0            19.0  understand, make, concept, example, work, clas...   \n",
       "4.0             3.0  great, learn, learning, clear, assignment, opp...   \n",
       "5.0             8.0  lot, recommend, highly, video, practical, teac...   \n",
       "6.0             8.0  lot, recommend, highly, video, practical, teac...   \n",
       "7.0             3.0  great, learn, learning, clear, assignment, opp...   \n",
       "8.0             3.0  great, learn, learning, clear, assignment, opp...   \n",
       "9.0             3.0  great, learn, learning, clear, assignment, opp...   \n",
       "\n",
       "     Num_Documents  Perc_Documents  \n",
       "0.0          176.0          0.0002  \n",
       "1.0          179.0          0.0002  \n",
       "2.0           12.0          0.0000  \n",
       "3.0       697589.0          0.7053  \n",
       "4.0            3.0          0.0000  \n",
       "5.0            NaN             NaN  \n",
       "6.0          953.0          0.0010  \n",
       "7.0           20.0          0.0000  \n",
       "8.0       157158.0          0.1589  \n",
       "9.0           15.0          0.0000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = posdf_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = posdf_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "posdf_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "posdf_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "posdf_dominant_topics.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
